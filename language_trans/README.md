# English to Ewe Translation Model

This repository contains the code and resources for a machine translation model that translates text from English to EWe. The model is built using TensorFlow and employs a sequence-to-sequence (Seq2Seq) architecture with LSTM layers. For better performance, you can use a Transformer-based model, but this implementation uses LSTM for simplicity.

## Table of Contents

- [Overview](#overview)
- [Data](#data)
- [Model](#model)
- [Training](#training)
- [Evaluation](#evaluation)
- [Usage](#usage)
- [Dependencies](#dependencies)
- [License](#license)

## Overview

Machine translation is the task of automatically converting text from one language to another. This project focuses on translating from English to [African Native Language]. The model is trained on a parallel corpus and leverages the Seq2Seq architecture with LSTM layers to perform the translation.

## Data

### Data Collection

The dataset should consist of parallel sentences in English and [African Native Language]. You can collect data from various sources such as books, websites, and translation projects. Ensure that the dataset is cleaned and preprocessed for optimal performance.

### Preprocessing

The data preprocessing involves:

- Tokenizing sentences
- Removing special characters
- Normalizing text

This step is crucial to ensure the model learns effectively from the data.

## Model

The model architecture is based on the Seq2Seq framework with LSTM layers. Here's a brief overview of the architecture:

- **Encoder**: Converts input English sentences into context vectors.
- **Decoder**: Converts context vectors into target [African Native Language] sentences.
- **Attention Mechanism**: Enhances the model by focusing on relevant parts of the input sentence during translation.

## Training

### Training Environment

Ensure you have the necessary libraries installed. You can use the provided `requirements.txt` to set up your environment:

```bash
pip install -r requirements.txt
```

### Training the Model

Use the following code to train the model:

```python
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense
from tensorflow.keras.models import Model

# Placeholder for dataset loading and preprocessing
def load_data():
    # Load your parallel corpus here
    return input_texts, target_texts

input_texts, target_texts = load_data()

# Tokenizer setup
input_tokenizer = tf.keras.preprocessing.text.Tokenizer()
target_tokenizer = tf.keras.preprocessing.text.Tokenizer()

input_tokenizer.fit_on_texts(input_texts)
target_tokenizer.fit_on_texts(target_texts)

input_sequences = input_tokenizer.texts_to_sequences(input_texts)
target_sequences = target_tokenizer.texts_to_sequences(target_texts)

max_input_len = max(len(seq) for seq in input_sequences)
max_target_len = max(len(seq) for seq in target_sequences)

input_vocab_size = len(input_tokenizer.word_index) + 1
target_vocab_size = len(target_tokenizer.word_index) + 1

# Pad sequences
input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_input_len, padding='post')
target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_target_len, padding='post')

# Define the model
def build_model(input_vocab_size, target_vocab_size, embedding_dim=256, units=512):
    encoder_inputs = Input(shape=(None,))
    enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
    encoder_lstm = LSTM(units, return_state=True)
    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(None,))
    dec_emb_layer = Embedding(target_vocab_size, embedding_dim)
    dec_emb = dec_emb_layer(decoder_inputs)
    decoder_lstm = LSTM(units, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
    decoder_dense = Dense(target_vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model

model = build_model(input_vocab_size, target_vocab_size)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

# Training
batch_size = 64
epochs = 50

target_sequences_input = target_sequences[:, :-1]
target_sequences_output = target_sequences[:, 1:]
target_sequences_output = tf.expand_dims(target_sequences_output, -1)

history = model.fit([input_sequences, target_sequences_input], target_sequences_output,
                    batch_size=batch_size, epochs=epochs, validation_split=0.2)

# Save model
model.save('nmt_model.h5')
```

## Evaluation

The model can be evaluated using metrics like BLEU and METEOR. These metrics help in understanding the quality of translations generated by the model.

## Usage

After training, you can use the model for inference. Here is an example of how to use the trained model to translate English text to [African Native Language]:

```python
# Load the model
model = tf.keras.models.load_model('nmt_model.h5')

# Function to translate new sentences
def translate(sentence, model, input_tokenizer, target_tokenizer, max_input_len, max_target_len):
    sequence = input_tokenizer.texts_to_sequences([sentence])
    sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_input_len, padding='post')
    prediction = model.predict([sequence, sequence])
    target_sequence = [np.argmax(word) for word in prediction[0]]
    target_text = target_tokenizer.sequences_to_texts([target_sequence])[0]
    return target_text

# Example usage
sentence = "Hello, how are you?"
translation = translate(sentence, model, input_tokenizer, target_tokenizer, max_input_len, max_target_len)
print(translation)
```

## Dependencies

- TensorFlow
- NumPy
- Other dependencies listed in `requirements.txt`

Install dependencies using:

```bash
pip install -r requirements.txt
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
