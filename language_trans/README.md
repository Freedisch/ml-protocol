# English to Ewe Translation Model

This repository contains the code and resources for a machine translation model that translates text from English to Ewe. The model is built using TensorFlow and employs a sequence-to-sequence (Seq2Seq) architecture with LSTM layers.

## Table of Contents

- [Overview](#overview)
- [Data](#data)
- [Model](#model)
- [Training](#training)
- [Evaluation](#evaluation)
- [Usage](#usage)
- [Dependencies](#dependencies)
- [License](#license)

## Overview

Machine translation is the task of automatically converting text from one language to another. This project focuses on translating from English to Ewe. The model is trained on a parallel corpus and leverages the Seq2Seq architecture with LSTM layers to perform the translation.

## Data

### Data Collection

The dataset `EWE_ENGLISH.csv` consists of parallel sentences in English and Ewe. This dataset is essential for training the machine translation model.

### Preprocessing

The data preprocessing involves:

- Tokenizing sentences
- Removing special characters
- Normalizing text

### Data Loading Example

```python
import pandas as pd

file_path = '/mnt/data/EWE_ENGLISH.csv'

# Attempt to read the CSV with error handling
try:
    data = pd.read_csv(file_path)
except pd.errors.ParserError as e:
    print(f"Error reading CSV: {e}")

# Drop unnecessary columns
data = data.drop(columns=['Unnamed: 0'])

# Display the first few rows to verify data integrity
data.head()
```

## Model

The model architecture is based on the Seq2Seq framework with LSTM layers. Here's a brief overview of the architecture:

- **Encoder**: Converts input English sentences into context vectors.
- **Decoder**: Converts context vectors into target Ewe sentences.
- **Attention Mechanism**: Enhances the model by focusing on relevant parts of the input sentence during translation.

### Model Building Example

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense
from tensorflow.keras.models import Model

def build_model(input_vocab_size, target_vocab_size, embedding_dim=256, units=512):
    encoder_inputs = Input(shape=(None,))
    enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
    encoder_lstm = LSTM(units, return_state=True)
    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
    encoder_states = [state_h, state_c]

    decoder_inputs = Input(shape=(None,))
    dec_emb_layer = Embedding(target_vocab_size, embedding_dim)
    dec_emb = dec_emb_layer(decoder_inputs)
    decoder_lstm = LSTM(units, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
    decoder_dense = Dense(target_vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model

model = build_model(input_vocab_size, target_vocab_size)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()
```

## Training

The model is trained with the specified batch size and epochs. Validation split is used to evaluate performance during training.

### Training Example

```python
batch_size = 64
epochs = 50

target_sequences_input = target_sequences[:, :-1]
target_sequences_output = target_sequences[:, 1:]
target_sequences_output = np.expand_dims(target_sequences_output, -1)

history = model.fit([input_sequences, target_sequences_input], target_sequences_output,
                    batch_size=batch_size, epochs=epochs, validation_split=0.2)

# Save model
model.save('nmt_model.h5')
```

## Evaluation

The model can be evaluated using metrics like BLEU and METEOR. These metrics help in understanding the quality of translations generated by the model.

## Usage

After training, you can use the model for inference. Here is an example of how to use the trained model to translate English text to Ewe:

### Translation Example

```python
def translate(sentence, model, input_tokenizer, target_tokenizer, max_input_len, max_target_len):
    sequence = input_tokenizer.texts_to_sequences([sentence])
    sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_input_len, padding='post')
    prediction = model.predict([sequence, sequence])
    target_sequence = [np.argmax(word) for word in prediction[0]]
    target_text = target_tokenizer.sequences_to_texts([target_sequence])[0]
    return target_text

# Example usage
sentence = "Hello, how are you?"
translation = translate(sentence, model, input_tokenizer, target_tokenizer, max_input_len, max_target_len)
print(f'Translation: {translation}')
```

## Dependencies

- TensorFlow
- NumPy
- Pandas

Install dependencies using:

```bash
pip install -r requirements.txt
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
